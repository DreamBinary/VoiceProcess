{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9804cc5814249e8",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1bd7ae61c78456a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c6dc3751338e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:32:09.666602900Z",
     "start_time": "2024-03-21T12:32:05.785148Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "bird_11-2_1595274334120 ['bird']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from env import DATA_PATH\n",
    "\n",
    "\n",
    "def load_dataset(dir_path: Path) -> (np.ndarray, np.ndarray):\n",
    "    data_files = list(dir_path.glob(\"*.wav\"))\n",
    "    features = []\n",
    "    labels = []\n",
    "    for file in data_files:\n",
    "        label = file.stem.split(\"_\")[0]\n",
    "\n",
    "        x, sr = librosa.load(file, sr=None)\n",
    "        mfccs = librosa.feature.mfcc(y=x, sr=sr, n_mfcc=40)\n",
    "        mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "        features.append(mfccs_scaled)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "\n",
    "def train(x, y):\n",
    "    model = SVC(kernel=\"linear\")\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def eval(model, x, y):\n",
    "    acc = model.score(x, y)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def test(model, dataset_path):\n",
    "    # random one\n",
    "    import random\n",
    "    import librosa\n",
    "    import numpy as np\n",
    "    files = list(dataset_path.glob(\"*.wav\"))\n",
    "    a_sound = files[random.randint(0, len(files))]\n",
    "    x, sr = librosa.load(a_sound, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=x, sr=sr, n_mfcc=40)\n",
    "    mfccs_scaled = np.mean(mfccs.T, axis=0)\n",
    "    print(a_sound.stem, model.predict([mfccs_scaled]))\n",
    "\n",
    "\n",
    "def main_svc():\n",
    "    dataset_path = DATA_PATH / \"animal\" / \"all\"\n",
    "    x, y = load_dataset(dataset_path)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    model = train(x_train, y_train)\n",
    "    acc = eval(model, x_test, y_test)\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "\n",
    "    test(model, dataset_path)\n",
    "\n",
    "\n",
    "main_svc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1979f9a8241b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transformer\n",
    "\n",
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:37:51.023226400Z",
     "start_time": "2024-03-21T12:37:49.006918600Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : model.py\n",
    "# @Time : 2024/3/20 17:48\n",
    "# @Author : fiv\n",
    "import math\n",
    "from typing import Callable\n",
    "from typing import Union\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F, TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_out=4, wav_length=512, d_model=40, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = True, norm_first: bool = False,\n",
    "                 bias: bool = True, device=None, dtype=None):\n",
    "        super(Transformer, self).__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.d_model = d_model\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                activation, layer_norm_eps, batch_first, norm_first,\n",
    "                                                bias, **factory_kwargs)\n",
    "        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        # self.embedding = nn.Embedding(n_out, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.linear = nn.Linear(d_model * wav_length, n_out)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        # self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bfd0b88519b0d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625d5880fa3b204b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:38:23.861536900Z",
     "start_time": "2024-03-21T12:38:22.271082300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : train.py\n",
    "# @Time : 2024/3/20 16:13\n",
    "# @Author : fiv\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from env import MODEL_PATH\n",
    "\n",
    "\n",
    "#\n",
    "def train(model, dataloader, total_run=10, output_path=MODEL_PATH / \"animal.pth\"):\n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    pbar = tqdm(range(total_run))\n",
    "    min_loss = 100\n",
    "    for _ in pbar:\n",
    "        loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            loss += loss.item()\n",
    "        loss = loss / len(dataloader)\n",
    "        pbar.set_description(f\"loss: {loss:.4f}\")\n",
    "        if loss < min_loss:\n",
    "            min_loss = loss\n",
    "            torch.save(model.state_dict(), output_path)\n",
    "    print(f\"Min loss: {min_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39acaf82e349f3a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7216baa7a9725aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:38:47.534723800Z",
     "start_time": "2024-03-21T12:38:46.593549800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : eval.py\n",
    "# @Time : 2024/3/21 16:31\n",
    "# @Author : fiv\n",
    "\n",
    "\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            output = model(x)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            # print(y, predicted)\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3debdb125a33a0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f575a4ee4ed110fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:39:26.634727400Z",
     "start_time": "2024-03-21T12:39:25.735869900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from script.preprocess import to_fbank\n",
    "\n",
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, dataset_dir=None):\n",
    "        self.label2idx = {\"bird\": 0, \"cat\": 1, \"dog\": 2, \"tiger\": 3}\n",
    "        self.idx2label = {v: k for k, v in self.label2idx.items()}\n",
    "        if dataset_dir is None:\n",
    "            from env import DATA_PATH\n",
    "            self.dataset_dir = DATA_PATH / \"animal\" / \"all\"\n",
    "        else:\n",
    "            self.dataset_dir = dataset_dir\n",
    "        self.file_path = list(self.dataset_dir.glob(\"*.wav\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fbank = to_fbank(self.file_path[idx])\n",
    "        length = 512\n",
    "        if fbank.shape[0] < length:\n",
    "            # repeat fbank to fill length\n",
    "            fbank = torch.cat([fbank] * (length // fbank.shape[0] + 1), dim=0)[:length]\n",
    "        else:\n",
    "            start = torch.randint(0, fbank.shape[0] - length, (1,))\n",
    "            fbank = fbank[start:start + length]\n",
    "        label = self.file_path[idx].stem.split(\"_\")[0]\n",
    "        return fbank, self.label2idx[label]\n",
    "\n",
    "    def idx2label(self, idx):\n",
    "        return self.idx2label[idx]\n",
    "\n",
    "\n",
    "def get_animal_dataloader(dataset_dir=None, batch_size=8, shuffle=True):\n",
    "    if dataset_dir is None:\n",
    "        from env import DATA_PATH\n",
    "        dataset_dir = DATA_PATH / \"animal\"\n",
    "\n",
    "    train_dir = dataset_dir / \"train\"\n",
    "    test_dir = dataset_dir / \"test\"\n",
    "\n",
    "    train = AnimalDataset(train_dir)\n",
    "    test = AnimalDataset(test_dir)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4b7bb5ce64ccc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:39:50.603108800Z",
     "start_time": "2024-03-21T12:39:49.439719200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2162: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss: 0.2162065953016281\n",
      "Accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_animal():\n",
    "    torch.cuda.empty_cache()\n",
    "    train_dataloader, test_dataloader = get_animal_dataloader()\n",
    "    output_path = MODEL_PATH / \"animal.pth\"\n",
    "    if output_path.exists():\n",
    "        output_path.unlink()\n",
    "    total_run = 1\n",
    "    model = Transformer(n_out=4, num_encoder_layers=1, dropout=0.2)\n",
    "    model = model.cuda()\n",
    "    train(model, train_dataloader, total_run, output_path)\n",
    "    model.load_state_dict(torch.load(output_path))\n",
    "    eval(model, test_dataloader)\n",
    "\n",
    "\n",
    "run_animal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc9aac61009555",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 狗叫测试集 from https://github.com/suzuki256/dog-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62513fc31511dc94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:41:19.132108500Z",
     "start_time": "2024-03-21T12:41:17.336396700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.6940: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min loss: 0.6939533352851868\n",
      "Accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DogDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.label2idx = {\"adult\": 0, \"dogs\": 1, \"puppy\": 2}\n",
    "        self.idx2label = {v: k for k, v in self.label2idx.items()}\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fbank = to_fbank(self.file_path[idx])\n",
    "        length = 512\n",
    "        if fbank.shape[0] < length:\n",
    "            # repeat fbank to fill length\n",
    "            fbank = torch.cat([fbank] * (length // fbank.shape[0] + 1), dim=0)[:length]\n",
    "        else:\n",
    "            start = torch.randint(0, fbank.shape[0] - length, (1,))\n",
    "            fbank = fbank[start:start + length]\n",
    "        label = self.file_path[idx].stem.split(\"_\")[0]\n",
    "        return fbank, self.label2idx[label]\n",
    "\n",
    "    def idx2label(self, idx):\n",
    "        return self.idx2label[idx]\n",
    "\n",
    "\n",
    "def get_dog_dataloader(batch_size=8, shuffle=True):\n",
    "    from script.util import split_dataset\n",
    "    from env import DATA_PATH\n",
    "    dataset_dir = DATA_PATH / \"dog\"\n",
    "    train_files, test_files = split_dataset(dataset_dir)\n",
    "    train_dataset = DogDataset(train_files)\n",
    "    test_dataset = DogDataset(test_files)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def run_animal():\n",
    "    torch.cuda.empty_cache()\n",
    "    train_dataloader, test_dataloader = get_animal_dataloader()\n",
    "    output_path = MODEL_PATH / \"animal.pth\"\n",
    "    if output_path.exists():\n",
    "        output_path.unlink()\n",
    "    total_run = 1\n",
    "    model = Transformer(n_out=4, num_encoder_layers=1, dropout=0.2)\n",
    "    model = model.cuda()\n",
    "    train(model, train_dataloader, total_run, output_path)\n",
    "    model.load_state_dict(torch.load(output_path))\n",
    "    eval(model, test_dataloader)\n",
    "\n",
    "\n",
    "run_animal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32d7735b570298c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 附"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53e665deb7a8570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:42:49.313194500Z",
     "start_time": "2024-03-21T12:42:49.290167600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : split.py\n",
    "# @Time : 2024/3/21 19:46\n",
    "# @Author : fiv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def split_dataset(dataset_dir: Path):\n",
    "    dataset_class = list(dataset_dir.glob(\"*\"))\n",
    "    # print(dataset_class)\n",
    "    train_files, test_files = [], []\n",
    "    for cla in dataset_class:\n",
    "        if cla == \"train\" or cla == \"test\":\n",
    "            continue\n",
    "        files_path = list(cla.glob(\"*\"))\n",
    "        train, test = train_test_split(files_path, test_size=0.2)\n",
    "        train_files.extend(train)\n",
    "        test_files.extend(test)\n",
    "    return train_files, test_files\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     print(split_dataset(Path(\"../../data/dog\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba4aa9e76d58d74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:42:45.377659600Z",
     "start_time": "2024-03-21T12:42:43.277548600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : to_fbank.py\n",
    "# @Time : 2024/3/20 14:14\n",
    "# @Author : fiv\n",
    "\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "\n",
    "\"\"\"\n",
    "Fbank：FilterBank：人耳对声音频谱的响应是非线性的，Fbank就是一种前端处理算法，\n",
    "以类似于人耳的方式对音频进行处理，可以提高语音识别的性能。\n",
    "获得语音信号的fbank特征的一般步骤是：预加重、分帧、加窗、短时傅里叶变换（STFT）、mel滤波、去均值等。\n",
    "对fbank做离散余弦变换（DCT）即可获得mfcc特征。\n",
    "\n",
    "MFCC(Mel-frequency cepstral coefficients):梅尔频率倒谱系数。\n",
    "梅尔频率是基于人耳听觉特性提出来的， 它与Hz频率成非线性对应关系。\n",
    "梅尔频率倒谱系数(MFCC)则是利用它们之间的这种关系，计算得到的Hz频谱特征。\n",
    "主要用于语音数据特征提取和降低运算维度。例如：对于一帧有512维(采样点)数据，\n",
    "经过MFCC后可以提取出最重要的40维(一般而言)数据同时也达到了降维的目的。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def to_fbank(wav_path: Path):\n",
    "    # from wav to fbank\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    fbank = torchaudio.compliance.kaldi.fbank(wav, num_mel_bins=40)\n",
    "    # fbank = fbank.unsqueeze(0)\n",
    "    return fbank\n",
    "\n",
    "# from env import DATA_PATH\n",
    "#\n",
    "# print(to_fbank(DATA_PATH / \"demo.wav\").shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94acc9935601d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
