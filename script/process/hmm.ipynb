{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c444476fc6a9e50d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "[TOC]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c10bac83ff5355",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 隐马尔科夫链求解词性标注\n",
    "\n",
    "本次实验使用了HMM、Transformer、CRF三种模型对中文词性标注进行了实验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc896918c2ac509",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:24:03.253991Z",
     "start_time": "2024-04-04T06:24:01.575770Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('今天', 't'), ('天气', 'n'), ('特别', 'd'), ('好', 'a')]\n",
      "[('欢迎', 'v'), ('大家', 'r'), ('的', 'u'), ('到来', 'vn')]\n",
      "[('请', 'v'), ('大家', 'r'), ('喝茶', 'v')]\n",
      "[('你', 'r'), ('的', 'u'), ('名字', 'n'), ('是', 'v'), ('什么', 'r')]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : hmm.py\n",
    "# @Time : 2024/3/26 20:16\n",
    "# @Author : fiv\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "# 隐马尔科夫链求解词性标注\n",
    "\n",
    "# pi[q] = 词性q出现所有句子开头的次数 / 所有句子的数量\n",
    "# trans[q1][q2] = 词性q1后面跟着词性q2的次数 / 词性q1出现的次数\n",
    "# emit[q][v] = 词性q发射出词v的次数 / 词性q出现的次数\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self, corpus_path):\n",
    "        # self.vocabs, self.classes = self.get_corpus(corpus_path)\n",
    "        self.corpus_path = corpus_path\n",
    "        self.line_cnt = 0\n",
    "        self.states = ['Ag', 'a', 'ad', 'an', 'Bg', 'b', 'c', 'Dg', 'd', 'e', 'f', 'h', 'i', 'j', 'k', 'l', 'Mg', 'm',\n",
    "                       'Ng', 'n', 'nr', 'ns', 'nt', 'nx', 'nz', 'o', 'p', 'q', 'Rg', 'r', 's', 'na', 'Tg', 't', 'u',\n",
    "                       'Vg', 'v', 'vd', 'vn', 'vvn', 'w', 'Yg', 'y', 'z']\n",
    "        self.pi = {state: 0.0 for state in self.states}  # 初始状态概率\n",
    "        self.trans = {state: {state: 0.0 for state in self.states} for state in self.states}  # 状态转移概率\n",
    "        self.emit = {state: {} for state in self.states}  # 发射概率\n",
    "        self.class_cnt = {state: 0 for state in self.states}\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        with open(self.corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip() for line in lines if line.strip()]\n",
    "            self.line_cnt = len(lines)\n",
    "            for line in lines:\n",
    "                vocabs, classes = [], []\n",
    "                words = line.split(\" \")\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    if '/' not in word:\n",
    "                        continue\n",
    "                    pos = word.index(\"/\")\n",
    "                    if '[' in word and ']' in word:\n",
    "                        vocabs.append(word[1:pos])\n",
    "                        classes.append(word[pos + 1:-1])\n",
    "                        break\n",
    "                    if '[' in word:\n",
    "                        vocabs.append(word[1:pos])\n",
    "                        classes.append(word[pos + 1:])\n",
    "                        break\n",
    "                    if ']' in word:\n",
    "                        vocabs.append(word[:pos])\n",
    "                        classes.append(word[pos + 1:-1])\n",
    "                        break\n",
    "                    vocabs.append(word[:pos])\n",
    "                    classes.append(word[pos + 1:])\n",
    "\n",
    "                assert len(vocabs) == len(classes)\n",
    "                self.pi[classes[0]] += 1\n",
    "                for v, c in zip(vocabs, classes):\n",
    "                    self.class_cnt[c] += 1\n",
    "                    if v in self.emit[c]:\n",
    "                        self.emit[c][v] += 1\n",
    "                    else:\n",
    "                        self.emit[c][v] = 1\n",
    "                for (c1, c2) in zip(classes[:-1], classes[1:]):\n",
    "                    self.trans[c1][c2] += 1\n",
    "\n",
    "        self.to_prob()\n",
    "\n",
    "    def to_prob(self):\n",
    "        for state in self.states:\n",
    "            self.pi[state] = self.pi[state] / self.line_cnt\n",
    "            for e in self.emit[state]:\n",
    "                self.emit[state][e] = self.emit[state][e] / self.class_cnt[state]\n",
    "            for t in self.trans[state]:\n",
    "                self.trans[state][t] = self.trans[state][t] / self.class_cnt[state]\n",
    "\n",
    "    def viterbi(self, sentence):\n",
    "        # 初始化\n",
    "        V = [{}]\n",
    "        path = {}\n",
    "\n",
    "        for y in self.states:\n",
    "            V[0][y] = self.pi[y] * self.emit[y].get(sentence[0], 0)\n",
    "            path[y] = [y]\n",
    "\n",
    "        # 递推\n",
    "        for t in range(1, len(sentence)):\n",
    "            V.append({})\n",
    "            newpath = {}\n",
    "\n",
    "            for y in self.states:\n",
    "                (prob, state) = max(\n",
    "                    (V[t - 1][y0] * self.trans[y0].get(y, 0) * self.emit[y].get(sentence[t], 0), y0) for y0 in\n",
    "                    self.states)\n",
    "                V[t][y] = prob\n",
    "                newpath[y] = path[state] + [y]\n",
    "            path = newpath\n",
    "\n",
    "        # 终止\n",
    "        (prob, state) = max((V[len(sentence) - 1][y], y) for y in self.states)\n",
    "        return prob, path[state]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hmm = HMM(\"../../data/corpus.txt\")\n",
    "    test_strs = [\"今天 天气 特别 好\", \"欢迎 大家 的 到来\", \"请 大家 喝茶\", \"你 的 名字 是 什么\"]\n",
    "    for s in test_strs:\n",
    "        ss = s.split(\" \")\n",
    "        p, o = hmm.viterbi(ss)\n",
    "        print(list(zip(ss, o)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc25cf0de011f03",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71aa72c143ed5fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : model.py\n",
    "# @Time : 2024/3/20 17:48\n",
    "# @Author : fiv\n",
    "import math\n",
    "from typing import Callable\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F, TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size=512, pos_tag_size=32, max_length=128, d_model=512, nhead: int = 8,\n",
    "                 num_encoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = True, norm_first: bool = False,\n",
    "                 bias: bool = True, device=None, dtype=None):\n",
    "        super(Transformer, self).__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.d_model = d_model\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                activation, layer_norm_eps, batch_first, norm_first,\n",
    "                                                bias, **factory_kwargs)\n",
    "        encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, bias=bias, **factory_kwargs)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.linear = nn.Linear(d_model, pos_tag_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"-----------------\")\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        x = self.pos_encoder(x)\n",
    "        # print(x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        # print(x.shape)\n",
    "        # print(\"-----------------\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61657cc721b511",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# pos dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d437c437bea81cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : dataset.py\n",
    "# @Time : 2024/3/27 9:16\n",
    "# @Author : fiv\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class POSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, vocabs, labels, max_length):\n",
    "        self.states = ['NONE', 'Ag', 'a', 'ad', 'an', 'Bg', 'b', 'c', 'Dg', 'd', 'e', 'f', 'h', 'i', 'j', 'k', 'l',\n",
    "                       'Mg', 'm', 'Ng', 'n', 'nr', 'ns', 'nt', 'nx', 'nz', 'o', 'p', 'q', 'Rg', 'r', 's', 'na', 'Tg',\n",
    "                       't', 'u', 'Vg', 'v', 'vd', 'vn', 'vvn', 'w', 'Yg', 'y', 'z']\n",
    "        self.label2idx = {state: idx for idx, state in enumerate(self.states)}\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\", use_fast=True)\n",
    "        self.corpus = [self.token_and_align_labels(vocab, label, max_length) for vocab, label in zip(vocabs, labels)]\n",
    "\n",
    "    def token_and_align_labels(self, tokens, labels, max_length):\n",
    "        tokens = self.tokenizer(tokens, truncation=True, is_split_into_words=True, padding=\"max_length\",\n",
    "                                add_special_tokens=False, max_length=max_length)\n",
    "        word_ids = tokens.word_ids()\n",
    "        aligned_labels = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned_labels.append(\"NONE\")\n",
    "            else:\n",
    "                aligned_labels.append(labels[wid])\n",
    "        # return tokens[\"input_ids\"], aligned_labels\n",
    "        return torch.tensor(tokens[\"input_ids\"]), torch.tensor([self.label2idx[label] for label in aligned_labels])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx]\n",
    "\n",
    "    def tag_size(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return self.tokenizer.vocab_size\n",
    "\n",
    "\n",
    "def get_data(corpus_path):\n",
    "    vocabs, classes = [], []\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        for line in lines:\n",
    "            vocab, label = [], []\n",
    "            words = line.split(\" \")\n",
    "            for word in words:\n",
    "                word = word.strip()\n",
    "                if '/' not in word:\n",
    "                    continue\n",
    "                pos = word.index(\"/\")\n",
    "                if '[' in word and ']' in word:\n",
    "                    vocab.append(word[1:pos])\n",
    "                    label.append(word[pos + 1:-1])\n",
    "                    break\n",
    "                if '[' in word:\n",
    "                    vocab.append(word[1:pos])\n",
    "                    label.append(word[pos + 1:])\n",
    "                    break\n",
    "                if ']' in word:\n",
    "                    vocab.append(word[:pos])\n",
    "                    label.append(word[pos + 1:-1])\n",
    "                    break\n",
    "                vocab.append(word[:pos])\n",
    "                label.append(word[pos + 1:])\n",
    "\n",
    "            assert len(vocab) == len(label)\n",
    "            vocabs.append(vocab)\n",
    "            classes.append(label)\n",
    "    return vocabs, classes\n",
    "\n",
    "\n",
    "def get_dataloader(corpus_path, max_length=128, batch_size=32):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    vocabs, classes = get_data(corpus_path)\n",
    "\n",
    "    # print(vocabs[0], classes[0])\n",
    "    train_vocabs, test_vocabs, train_classes, test_classes = train_test_split(vocabs, classes, test_size=0.2)\n",
    "    train_dataset = POSDataset(train_vocabs, train_classes, max_length)\n",
    "    test_dataset = POSDataset(test_vocabs, test_classes, max_length)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d8f2b6dcce65f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4764cd5cbee1b260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:32:44.191841Z",
     "start_time": "2024-04-04T06:32:44.163865Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# @FileName : crf.py\n",
    "# @Time : 2024/4/4 13:13\n",
    "# @Author : fiv\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import os\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "\n",
    "# 基于条件随机场的词性标注\n",
    "\n",
    "class CRF_POS:\n",
    "    def __init__(self, corpus_path):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.line_cnt = 0\n",
    "        self.states = ['Ag', 'a', 'ad', 'an', 'Bg', 'b', 'c', 'Dg', 'd', 'e', 'f', 'h', 'i', 'j', 'k', 'l', 'Mg', 'm',\n",
    "                       'Ng', 'n', 'nr', 'ns', 'nt', 'nx', 'nz', 'o', 'p', 'q', 'Rg', 'r', 's', 'na', 'Tg', 't', 'u',\n",
    "                       'Vg', 'v', 'vd', 'vn', 'vvn', 'w', 'Yg', 'y', 'z']\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.crf = CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=10,\n",
    "            all_possible_transitions=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        # * ``'lbfgs'`` - Gradient descent using the L-BFGS method   -->> 梯度下降\n",
    "        # * ``'l2sgd'`` - Stochastic Gradient Descent with L2 regularization term  -->> 随机梯度下降\n",
    "        # * ``'ap'`` - Averaged Perceptron  -->> 感知机\n",
    "        # * ``'pa'`` - Passive Aggressive (PA)  -->> 消极攻击\n",
    "        # * ``'arow'`` - Adaptive Regularization Of Weight Vector (AROW)  -->> 自适应正则化权重向量\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        with open(self.corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip() for line in lines if line.strip()]\n",
    "            self.line_cnt = len(lines)\n",
    "            for line in lines:\n",
    "                vocabs, classes = [], []\n",
    "                words = line.split(\" \")\n",
    "                for word in words:\n",
    "                    word = word.strip()\n",
    "                    if '/' not in word:\n",
    "                        continue\n",
    "                    pos = word.index(\"/\")\n",
    "                    if '[' in word and ']' in word:\n",
    "                        vocabs.append(word[1:pos])\n",
    "                        classes.append(word[pos + 1:-1])\n",
    "                        break\n",
    "                    if '[' in word:\n",
    "                        vocabs.append(word[1:pos])\n",
    "                        classes.append(word[pos + 1:])\n",
    "                        break\n",
    "                    if ']' in word:\n",
    "                        vocabs.append(word[:pos])\n",
    "                        classes.append(word[pos + 1:-1])\n",
    "                        break\n",
    "                    vocabs.append(word[:pos])\n",
    "                    classes.append(word[pos + 1:])\n",
    "                assert len(vocabs) == len(classes)\n",
    "                self.X.append(vocabs)\n",
    "                self.y.append(classes)\n",
    "        self.crf.fit(self.X, self.y)\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        vocabs = sentence.split(\" \")\n",
    "        return self.crf.predict([vocabs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e5aebc501acad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:33:27.688403Z",
     "start_time": "2024-04-04T06:32:44.616929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 19484/19484 [00:00<00:00, 20752.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 22432\n",
      "Seconds required: 0.271\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.100000\n",
      "c2: 0.100000\n",
      "num_memories: 6\n",
      "max_iterations: 10\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=5.10  loss=2956295.20 active=22181 feature_norm=1.00\n",
      "Iter 2   time=12.92 loss=1971237.48 active=22178 feature_norm=13.06\n",
      "Iter 3   time=2.59  loss=1702008.52 active=20991 feature_norm=14.64\n",
      "Iter 4   time=2.61  loss=1532821.93 active=21818 feature_norm=17.88\n",
      "Iter 5   time=5.27  loss=1341643.23 active=21740 feature_norm=26.51\n",
      "Iter 6   time=2.62  loss=1232498.37 active=22048 feature_norm=28.99\n",
      "Iter 7   time=2.58  loss=1185865.87 active=22232 feature_norm=30.61\n",
      "Iter 8   time=2.42  loss=1073502.87 active=21911 feature_norm=37.36\n",
      "Iter 9   time=2.39  loss=963623.78 active=22015 feature_norm=45.49\n",
      "Iter 10  time=2.40  loss=855361.18 active=21968 feature_norm=55.28\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 40.904\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 21968 (22432)\n",
      "Number of active attributes: 4538 (4538)\n",
      "Number of active labels: 44 (44)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.007\n"
     ]
    }
   ],
   "source": [
    "crf = CRF_POS(\"../../data/corpus.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57baf0c2fbc0f9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T06:33:34.885266Z",
     "start_time": "2024-04-04T06:33:34.866431Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('今天', 't'), ('天气', 't'), ('特别', 'd'), ('好', 'a')]\n",
      "[('欢迎', 'd'), ('大家', 'a'), ('的', 'u'), ('到来', 'v')]\n",
      "[('请', 'd'), ('大家', 'a'), ('喝茶', 'u')]\n",
      "[('你', 'v'), ('的', 'u'), ('名字', 'n'), ('是', 'v'), ('什么', 'r')]\n"
     ]
    }
   ],
   "source": [
    "test_strs = [\"今天 天气 特别 好\", \"欢迎 大家 的 到来\", \"请 大家 喝茶\", \"你 的 名字 是 什么\"]\n",
    "for test_str in test_strs:\n",
    "    print(list(zip(test_str.split(\" \"), crf.predict(test_str)[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
